import argparse
import json
import re
import types
from pathlib import Path
from typing import Dict, Iterable, List, Tuple

import torch

import chat_pipeline
from chat_web_app import Engine


def _js_regex_flags(py_flags: int) -> str:
    flags = ""
    if py_flags & re.IGNORECASE:
        flags += "i"
    if py_flags & re.MULTILINE:
        flags += "m"
    if py_flags & re.DOTALL:
        flags += "s"
    return flags


def _serialize_regex(rx: re.Pattern) -> Dict[str, str]:
    return {"pattern": rx.pattern, "flags": _js_regex_flags(rx.flags)}


def _sorted_strings(values: Iterable[str]) -> List[str]:
    return sorted(str(v) for v in values)


def generate_feature_js(out_path: Path) -> None:
    regex_names = [
        "TOKEN_RE",
        "PROGRAMMING_HINT_RE",
        "SCIENCE_HINT_RE",
        "ENGLISH_HINT_RE",
        "MATH_HINT_RE",
        "SCRIPTURE_REF_RE",
        "SCRIPTURE_HINT_RE",
        "DICTIONARY_HINT_RE",
        "CODE_HINT_RE",
        "ROLE_LINE_RE",
        "IDENTIFIER_TOKEN_RE",
        "CAMEL_OR_WORD_RE",
        "CODE_LINE_SHAPE_WORD_RE",
    ]
    regex_cfg = {name: _serialize_regex(getattr(chat_pipeline, name)) for name in regex_names}

    set_names = [
        "PROGRAMMING_WORDS",
        "CREATIVE_WORDS",
        "ANALYTIC_WORDS",
        "SCIENCE_WORDS",
        "ENGLISH_WORDS",
        "MATH_WORDS",
        "SCRIPTURE_WORDS",
        "DICTIONARY_WORDS",
    ]
    sets_cfg = {name: _sorted_strings(getattr(chat_pipeline, name)) for name in set_names}

    payload = {
        "feat_dim": int(chat_pipeline.FEAT_DIM),
        "model_classes": int(chat_pipeline.MODEL_CLASSES),
        "regex": regex_cfg,
        "sets": sets_cfg,
    }
    cfg_json = json.dumps(payload, ensure_ascii=False, separators=(",", ":"))

    # Exact text featurizer + context_v2 port (for feature_mode=context_v2 / legacy).
    js = """// Generated by export_chat_browser_onnx.py. Do not hand-edit.
(function(global) {{
  const CFG = __CFG_JSON__;
  const FEAT_DIM = CFG.feat_dim;
  const MODEL_CLASSES = CFG.model_classes;
  const RX = {{}};
  for (const [k, v] of Object.entries(CFG.regex)) {{
    RX[k] = new RegExp(v.pattern, v.flags);
  }}
  const SETS = {{}};
  for (const [k, arr] of Object.entries(CFG.sets)) {{
    SETS[k] = new Set(arr);
  }}

  const enc = new TextEncoder();
  const _featCache = new Map();
  const _textTmp = new Float32Array(FEAT_DIM);

  function _requireBlake() {{
    if (typeof global.blake2b === 'function') return global.blake2b;
    if (global.blakejs && typeof global.blakejs.blake2b === 'function') return global.blakejs.blake2b;
    throw new Error('blake2b function not found. Load blakejs before chat_feature_ctxv2.generated.js');
  }}

  function _stableHash64(text) {{
    const blake2b = _requireBlake();
    const out = blake2b(enc.encode(String(text)), null, 8);
    let h = 0n;
    for (let i = 0; i < 8; i++) {{
      h |= BigInt(out[i]) << BigInt(8 * i); // little-endian
    }}
    return h;
  }}

  function _hashAdd(vec, key, weight, dim = FEAT_DIM, signBit = 1) {{
    const h = _stableHash64(key);
    const idx = Number(h % BigInt(dim));
    const sign = (((h >> BigInt(signBit)) & 1n) === 0n) ? 1.0 : -1.0;
    vec[idx] += Number(weight) * sign;
  }}

  function _tokens(text, maxTokens = 384) {{
    const m = String(text || '').toLowerCase().match(RX.TOKEN_RE);
    if (!m) return [];
    return m.slice(0, maxTokens);
  }}

  function _identifierSubtokens(token, maxParts = 8) {{
    const raw = String(token || '').replace(/^[`'"]+|[`'"]+$/g, '');
    if (!raw) return [];
    const chunks = raw.split(/[_\\-./:]+/);
    const parts = [];
    for (const chunk0 of chunks) {{
      const chunk = chunk0.trim();
      if (!chunk) continue;
      const matches = chunk.match(RX.CAMEL_OR_WORD_RE);
      if (matches && matches.length) parts.push(...matches);
      else parts.push(chunk);
    }}
    const out = [];
    for (const p0 of parts) {{
      const p = String(p0).toLowerCase().trim();
      if (p.length >= 2) out.push(p);
      if (out.length >= maxParts) break;
    }}
    return out;
  }}

  function _looksCodeLikeLine(line) {{
    const s = String(line || '').replace(/\\r?$/, '');
    if (!s) return false;
    if (s.startsWith('```')) return true;
    if (s.startsWith('    ') || s.startsWith('\\t')) return true;
    if (/\\b(def|class|import|from|return|if|else|for|while|try|except|catch|function|const|let|var|SELECT|INSERT|UPDATE|DELETE)\\b/.test(s)) return true;
    const punct = (s.match(/[{}();]/g) || []).length;
    if (punct >= 2) return true;
    if (s.includes('=>') || s.includes('::') || s.includes('->')) return true;
    return false;
  }}

  function _zeros() {{
    return new Float32Array(FEAT_DIM);
  }}

  function _l2Normalize(vec) {{
    let ss = 0.0;
    for (let i = 0; i < vec.length; i++) ss += vec[i] * vec[i];
    if (ss <= 0) return vec;
    const inv = 1.0 / Math.sqrt(ss);
    for (let i = 0; i < vec.length; i++) vec[i] *= inv;
    return vec;
  }}

  function _cloneVec(vec) {{
    return new Float32Array(vec);
  }}

  function _addScaled(acc, vec, w) {{
    for (let i = 0; i < acc.length; i++) acc[i] += vec[i] * w;
    return acc;
  }}

  function _featurizeTextImpl(text, dim = FEAT_DIM) {{
    if (dim !== FEAT_DIM) throw new Error('Generated browser encoder only supports dim=128');
    const vec = _zeros();
    const toks = _tokens(text);
    if (!toks.length) return vec;

    for (const tok of toks) {{
      const h = _stableHash64('u|' + tok);
      const idx = Number(h % BigInt(dim));
      const sign = (((h >> 1n) & 1n) === 0n) ? 1.0 : -1.0;
      vec[idx] += sign;
    }}

    for (let i = 0; i < toks.length - 1; i++) {{
      const bg = toks[i] + '__' + toks[i + 1];
      const h = _stableHash64('b|' + bg);
      const idx = Number(h % BigInt(dim));
      const sign = (((h >> 2n) & 1n) === 0n) ? 1.0 : -1.0;
      vec[idx] += 0.75 * sign;
    }}

    let codeHits = 0, creativeHits = 0, analyticHits = 0, scienceHits = 0;
    let englishHits = 0, mathHits = 0, scriptureHits = 0, dictHits = 0;

    for (const tok of toks) {{
      if (SETS.PROGRAMMING_WORDS.has(tok)) {{
        codeHits += 1;
        _hashAdd(vec, 'kw|code|' + tok, 1.10, dim, 3);
      }}
      if (SETS.CREATIVE_WORDS.has(tok)) {{
        creativeHits += 1;
        _hashAdd(vec, 'kw|creative|' + tok, 0.55, dim, 4);
      }}
      if (SETS.ANALYTIC_WORDS.has(tok)) {{
        analyticHits += 1;
        _hashAdd(vec, 'kw|analytic|' + tok, 0.55, dim, 5);
      }}
      if (SETS.SCIENCE_WORDS.has(tok)) {{
        scienceHits += 1;
        _hashAdd(vec, 'kw|science|' + tok, 0.82, dim, 46);
      }}
      if (SETS.ENGLISH_WORDS.has(tok)) {{
        englishHits += 1;
        _hashAdd(vec, 'kw|english|' + tok, 0.75, dim, 26);
      }}
      if (SETS.MATH_WORDS.has(tok)) {{
        mathHits += 1;
        _hashAdd(vec, 'kw|math|' + tok, 0.90, dim, 27);
      }}
      if (SETS.SCRIPTURE_WORDS.has(tok)) {{
        scriptureHits += 1;
        _hashAdd(vec, 'kw|scripture|' + tok, 0.80, dim, 64);
      }}
      if (SETS.DICTIONARY_WORDS.has(tok)) {{
        dictHits += 1;
        _hashAdd(vec, 'kw|dictionary|' + tok, 0.72, dim, 62);
      }}
    }}

    const rawIdentifiers = String(text || '').match(RX.IDENTIFIER_TOKEN_RE) || [];
    for (const ident of rawIdentifiers.slice(0, 96)) {{
      const subtoks = _identifierSubtokens(ident, 8);
      if (!subtoks.length) continue;
      if (subtoks.length >= 2) codeHits += 1;
      for (const sub of subtoks) _hashAdd(vec, 'id|' + sub, 0.40, dim, 6);
      if (ident.length >= 5) {{
        _hashAdd(vec, 'idh|' + ident.slice(0, 4).toLowerCase(), 0.18, dim, 7);
        _hashAdd(vec, 'idt|' + ident.slice(-4).toLowerCase(), 0.18, dim, 8);
      }}
    }}

    const textStr = String(text || '');
    if (RX.PROGRAMMING_HINT_RE.test(textStr)) {{
      codeHits += 2;
      _hashAdd(vec, 'domain|code_hint', 0.90, dim, 9);
    }}
    if (RX.SCIENCE_HINT_RE.test(textStr)) {{
      scienceHits += 2;
      _hashAdd(vec, 'domain|science_hint', 0.78, dim, 47);
    }}
    if (RX.ENGLISH_HINT_RE.test(textStr)) {{
      englishHits += 2;
      _hashAdd(vec, 'domain|english_hint', 0.70, dim, 28);
    }}
    if (RX.MATH_HINT_RE.test(textStr)) {{
      mathHits += 2;
      _hashAdd(vec, 'domain|math_hint', 0.85, dim, 29);
    }}
    if (RX.SCRIPTURE_HINT_RE.test(textStr) || RX.SCRIPTURE_REF_RE.test(textStr)) {{
      scriptureHits += 2;
      _hashAdd(vec, 'domain|scripture_hint', 0.78, dim, 65);
    }}
    if (RX.DICTIONARY_HINT_RE.test(textStr)) {{
      dictHits += 2;
      _hashAdd(vec, 'domain|dictionary_hint', 0.72, dim, 63);
    }}

    let codeLikeLines = 0;
    for (const line of textStr.split(/\\r?\\n/).slice(0, 16)) {{
      if (!_looksCodeLikeLine(line)) continue;
      codeLikeLines += 1;
      let shape = line.trim();
      shape = shape.replace(RX.CODE_LINE_SHAPE_WORD_RE, 'ID');
      shape = shape.replace(/\\d+/g, 'N');
      shape = shape.replace(/"[^"]*"|'[^']*'/g, 'STR');
      shape = shape.replace(/\\s+/g, ' ').slice(0, 96);
      if (shape) _hashAdd(vec, 'line|' + shape, 0.65, dim, 10);
    }}

    if (codeHits > 0 || codeLikeLines > 0) _hashAdd(vec, 'domain|code', Math.min(1.25, 0.18 * (codeHits + codeLikeLines)), dim, 11);
    if (scienceHits > 0) _hashAdd(vec, 'domain|science', Math.min(1.0, 0.15 * scienceHits), dim, 48);
    if (englishHits > 0) _hashAdd(vec, 'domain|english', Math.min(0.95, 0.14 * englishHits), dim, 30);
    if (mathHits > 0) _hashAdd(vec, 'domain|math', Math.min(1.05, 0.16 * mathHits), dim, 31);
    if (scriptureHits > 0) _hashAdd(vec, 'domain|scripture', Math.min(1.0, 0.15 * scriptureHits), dim, 66);
    if (dictHits > 0) _hashAdd(vec, 'domain|dictionary', Math.min(0.95, 0.14 * dictHits), dim, 53);
    if (creativeHits > 0) _hashAdd(vec, 'domain|creative', Math.min(0.70, 0.12 * creativeHits), dim, 12);
    if (analyticHits > 0) _hashAdd(vec, 'domain|analytic', Math.min(0.70, 0.12 * analyticHits), dim, 13);

    return _l2Normalize(vec);
  }}

  function featurizeText(text, dim = FEAT_DIM) {{
    if (dim !== FEAT_DIM) throw new Error('Generated browser encoder only supports dim=128');
    const clean = String(text || '').trim();
    if (!clean) return _zeros();
    const hit = _featCache.get(clean);
    if (hit) return _cloneVec(hit);
    const vec = _featurizeTextImpl(clean, dim);
    if (_featCache.size > 50000) _featCache.clear();
    _featCache.set(clean, _cloneVec(vec));
    return vec;
  }}

  function _splitRoleLine(line) {{
    const m = RX.ROLE_LINE_RE.exec(String(line || '').trim());
    if (!m) return ['text', String(line || '').trim()];
    return [String(m[1]).trim().toLowerCase(), String(m[2] || '').trim()];
  }}

  function featurizeContextV2(contextText, dim = FEAT_DIM, maxLines = 32) {{
    if (dim !== FEAT_DIM) throw new Error('Generated browser encoder only supports dim=128');
    const raw = String(contextText || '').trim();
    if (!raw) return _zeros();
    let lines = raw.split(/\\r?\\n/).map(s => s.trim()).filter(Boolean);
    if (!lines.length) return featurizeText(raw, dim);
    if (lines.length > maxLines) lines = lines.slice(-maxLines);

    const roleWeights = {{
      user: 1.20,
      assistant: 0.95,
      system: 0.85,
      tool: 0.88,
      memory_user: 1.05,
      memory_assistant: 0.90,
      text: 1.00,
    }};
    const acc = _zeros();
    const n = lines.length;
    const questionBoost = featurizeText('[question_intent]', dim);
    const actionBoost = featurizeText('[action_request]', dim);

    for (let idx = 0; idx < lines.length; idx++) {{
      const [role0, content] = _splitRoleLine(lines[idx]);
      if (!content) continue;
      let roleKey = role0;
      if (role0.startsWith('memory ') && role0.includes(' user')) roleKey = 'memory_user';
      else if (role0.startsWith('memory ') && role0.includes(' assistant')) roleKey = 'memory_assistant';
      if (!(roleKey in roleWeights)) roleKey = 'text';

      const recency = 0.55 + 0.45 * (idx + 1) / Math.max(1, n);
      const weight = recency * roleWeights[roleKey];
      _addScaled(acc, featurizeText('[' + roleKey + '] ' + content, dim), weight);

      const lower = content.toLowerCase();
      if (content.includes('?')) _addScaled(acc, questionBoost, 0.035);
      if (/\\b(please|can you|could you|help|build|fix|make|optimi[sz]e)\\b/.test(lower)) {{
        _addScaled(acc, actionBoost, 0.030);
      }}
    }}
    return _l2Normalize(acc);
  }}

  function buildContext(history, userText, maxTurns = 4) {{
    const parts = [];
    const h = Array.isArray(history) ? history : [];
    for (const turn of h.slice(-Math.max(0, maxTurns))) {{
      if (Array.isArray(turn)) {{
        parts.push('User: ' + (turn[0] || ''));
        parts.push('Assistant: ' + (turn[1] || ''));
      }} else if (turn && typeof turn === 'object') {{
        parts.push('User: ' + (turn.user || ''));
        parts.push('Assistant: ' + (turn.bot || turn.assistant || ''));
      }}
    }}
    parts.push('User: ' + (userText || ''));
    return parts.join('\\n');
  }}

  function textToModelInput(text, featureMode = 'legacy') {{
    const mode = String(featureMode || 'legacy').trim().toLowerCase();
    const vec = (mode === 'context_v2') ? featurizeContextV2(text, FEAT_DIM) : featurizeText(text, FEAT_DIM);
    return vec; // Float32Array length 128
  }}

  global.ChatFeatureCtxV2 = {{
    FEAT_DIM,
    MODEL_CLASSES,
    featurizeText,
    featurizeContextV2,
    buildContext,
    textToModelInput,
    _version: 'ctxv2-js-1',
  }};
}})(typeof globalThis !== 'undefined' ? globalThis : window);
""".replace("{{", "{").replace("}}", "}").replace("__CFG_JSON__", cfg_json)
    out_path.write_text(js, encoding="utf-8")


def export_chat_onnx(weights: Path, meta: Path, out_onnx: Path) -> Dict[str, object]:
    engine = Engine(
        device=torch.device("cpu"),
        device_info={"resolved": "cpu"},
        defaults={
            "model_size": "auto",
            "max_turns": 2,
            "top_labels": 3,
            "pool_mode": "all",
            "response_temperature": 0.08,
            "temperature": 0.0,
            "style_mode": "auto",
            "creativity": 0.2,
        },
    )
    status = engine.load(str(weights), str(meta))
    model = engine.model
    if model is None:
        raise RuntimeError("Model failed to load for ONNX export")
    _patch_export_safe_layers(model)

    out_onnx.parent.mkdir(parents=True, exist_ok=True)
    dummy = torch.randn(1, 1, chat_pipeline.FEAT_DIM, dtype=torch.float32)
    torch.onnx.export(
        model.cpu(),
        dummy,
        str(out_onnx),
        input_names=["x"],
        output_names=["logits"],
        opset_version=17,
        dynamic_axes={"x": {0: "batch", 1: "seq"}, "logits": {0: "batch", 1: "seq"}},
        dynamo=False,
    )
    return status


def _patch_export_safe_layers(model: torch.nn.Module) -> None:
    for module in model.modules():
        # run.SaliencyPrune is trace-unfriendly because it converts Tensor shape math through round(int).
        if not (
            hasattr(module, "saliency_predictor")
            and hasattr(module, "prune_frac")
            and hasattr(module, "norm")
            and hasattr(module, "layer_scale")
        ):
            continue
        try:
            d_model = int(module.norm.weight.numel())
            prune_frac = float(module.prune_frac)
        except Exception:
            continue
        fixed_k = max(1, min(d_model, int(round(d_model * (1.0 - prune_frac)))))

        def _forward_export_safe(self, x, _fixed_k=fixed_k):
            residual = x
            x = self.norm(x)
            s = torch.sigmoid(self.saliency_predictor(x))
            _, idx = torch.topk(s, k=_fixed_k, dim=-1)
            mask = torch.zeros_like(x).scatter_(-1, idx, 1.0)
            x = x * mask
            x = self.layer_scale(x)
            return residual + x

        module.forward = types.MethodType(_forward_export_safe, module)


def main() -> None:
    ap = argparse.ArgumentParser(description="Export supermix chat model for browser ONNX mode and generate JS context_v2 encoder.")
    ap.add_argument("--weights", default="champion_model_chat_supermix_v27_500k_ft.pth")
    ap.add_argument("--meta", default="chat_model_meta_supermix_v27_500k.json")
    ap.add_argument("--out-onnx", default="github_pages_chat/champion_model_chat_supermix_v27_500k_ft.onnx")
    ap.add_argument("--out-feature-js", default="github_pages_chat/chat_feature_ctxv2.generated.js")
    ap.add_argument("--copy-meta-to", default="github_pages_chat/chat_model_meta_supermix_v27_500k.json")
    args = ap.parse_args()

    weights = Path(args.weights)
    meta = Path(args.meta)
    out_onnx = Path(args.out_onnx)
    out_feature_js = Path(args.out_feature_js)
    copy_meta_to = Path(args.copy_meta_to) if args.copy_meta_to else None

    if not weights.exists():
        raise FileNotFoundError(f"Weights not found: {weights}")
    if not meta.exists():
        raise FileNotFoundError(f"Metadata not found: {meta}")

    print(f"Exporting ONNX from {weights} using metadata {meta} ...")
    status = export_chat_onnx(weights=weights, meta=meta, out_onnx=out_onnx)
    print("ONNX export complete:", out_onnx, "bytes=", out_onnx.stat().st_size)
    print("Loaded status:", json.dumps(status, indent=2))

    print("Generating JS feature encoder:", out_feature_js)
    out_feature_js.parent.mkdir(parents=True, exist_ok=True)
    generate_feature_js(out_feature_js)
    print("Generated:", out_feature_js, "bytes=", out_feature_js.stat().st_size)

    if copy_meta_to:
        copy_meta_to.parent.mkdir(parents=True, exist_ok=True)
        if meta.resolve() != copy_meta_to.resolve():
            copy_meta_to.write_bytes(meta.read_bytes())
        print("Metadata copy:", copy_meta_to, "bytes=", copy_meta_to.stat().st_size)


if __name__ == "__main__":
    main()
